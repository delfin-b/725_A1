step 0: classification- train loss 1.0548, classification-val loss 1.0546
[WANDB] Logging at iter 0: train 1.054772973060608, val 1.0546143054962158
iter 0: loss 1.0640, time 5235.97ms, mfu -100.00%
iter 1: loss 1.0850, time 3063.77ms, mfu -100.00%
iter 2: loss 1.0244, time 3100.05ms, mfu -100.00%
iter 3: loss 1.1062, time 3164.64ms, mfu -100.00%
iter 4: loss 1.0505, time 3124.63ms, mfu -100.00%
iter 5: loss 1.0225, time 3140.77ms, mfu 0.45%
iter 6: loss 1.0322, time 3110.76ms, mfu 0.45%
iter 7: loss 0.9895, time 3112.49ms, mfu 0.45%
iter 8: loss 0.9905, time 3123.65ms, mfu 0.45%
iter 9: loss 0.9846, time 3179.65ms, mfu 0.45%
step 10: classification- train loss 0.9175, classification-val loss 0.9261
[WANDB] Logging at iter 10: train 0.91754150390625, val 0.9261108636856079
saving checkpoint to out-sentiment-small
iter 10: loss 0.9868, time 4750.70ms, mfu 0.44%
iter 11: loss 0.9453, time 3145.83ms, mfu 0.44%
iter 12: loss 0.9478, time 3222.59ms, mfu 0.44%
iter 13: loss 0.8525, time 3304.15ms, mfu 0.44%
iter 14: loss 0.9048, time 3200.24ms, mfu 0.44%
iter 15: loss 0.8108, time 3200.47ms, mfu 0.44%
iter 16: loss 0.8254, time 3209.18ms, mfu 0.44%
iter 17: loss 0.9114, time 3147.42ms, mfu 0.44%
iter 18: loss 0.7617, time 3195.07ms, mfu 0.44%
iter 19: loss 0.7703, time 3158.41ms, mfu 0.44%
step 20: classification- train loss 0.7600, classification-val loss 0.7698
[WANDB] Logging at iter 20: train 0.759967029094696, val 0.769824206829071
saving checkpoint to out-sentiment-small
iter 20: loss 0.7456, time 4669.13ms, mfu 0.43%
iter 21: loss 0.7866, time 3188.34ms, mfu 0.43%
iter 22: loss 0.7588, time 3181.85ms, mfu 0.43%
iter 23: loss 0.7959, time 3150.58ms, mfu 0.43%
iter 24: loss 0.8892, time 3143.46ms, mfu 0.43%
iter 25: loss 0.6479, time 3196.59ms, mfu 0.43%
iter 26: loss 0.7292, time 3179.25ms, mfu 0.44%
iter 27: loss 0.7278, time 3260.19ms, mfu 0.44%
iter 28: loss 0.9146, time 3164.94ms, mfu 0.44%
iter 29: loss 0.6633, time 3164.62ms, mfu 0.44%
step 30: classification- train loss 0.7574, classification-val loss 0.7531
[WANDB] Logging at iter 30: train 0.757366955280304, val 0.7530883550643921
saving checkpoint to out-sentiment-small
iter 30: loss 0.6991, time 4802.06ms, mfu 0.42%
iter 31: loss 0.7051, time 3181.16ms, mfu 0.43%
iter 32: loss 0.6365, time 3237.03ms, mfu 0.43%
iter 33: loss 0.7043, time 3196.65ms, mfu 0.43%
iter 34: loss 0.6799, time 3206.81ms, mfu 0.43%
iter 35: loss 0.7322, time 3209.11ms, mfu 0.43%
iter 36: loss 0.6008, time 3162.41ms, mfu 0.43%
iter 37: loss 0.6206, time 3196.96ms, mfu 0.43%
iter 38: loss 0.6217, time 3215.91ms, mfu 0.43%
iter 39: loss 0.6678, time 3194.61ms, mfu 0.44%
step 40: classification- train loss 0.7129, classification-val loss 0.7297
[WANDB] Logging at iter 40: train 0.7129150629043579, val 0.7296692132949829
saving checkpoint to out-sentiment-small
iter 40: loss 0.8873, time 4888.03ms, mfu 0.42%
iter 41: loss 0.6589, time 3269.12ms, mfu 0.42%
iter 42: loss 0.6416, time 3190.08ms, mfu 0.42%
iter 43: loss 0.7008, time 3214.85ms, mfu 0.43%
iter 44: loss 0.7024, time 3188.85ms, mfu 0.43%
iter 45: loss 0.6252, time 3246.38ms, mfu 0.43%
iter 46: loss 0.8392, time 3178.94ms, mfu 0.43%
iter 47: loss 0.6106, time 3185.67ms, mfu 0.43%
iter 48: loss 0.6494, time 3205.39ms, mfu 0.43%
iter 49: loss 0.6086, time 3199.94ms, mfu 0.43%
step 50: classification- train loss 0.6181, classification-val loss 0.6314
[WANDB] Logging at iter 50: train 0.6181389093399048, val 0.6314407587051392
saving checkpoint to out-sentiment-small
iter 50: loss 0.5403, time 4657.45ms, mfu 0.42%
iter 51: loss 0.9150, time 3199.35ms, mfu 0.42%
iter 52: loss 0.6406, time 3191.75ms, mfu 0.42%
iter 53: loss 0.4772, time 3189.62ms, mfu 0.43%
iter 54: loss 0.5478, time 3257.47ms, mfu 0.43%
iter 55: loss 0.5051, time 3177.00ms, mfu 0.43%
iter 56: loss 0.4353, time 3187.94ms, mfu 0.43%
iter 57: loss 0.4311, time 3308.01ms, mfu 0.43%
iter 58: loss 0.4734, time 3198.28ms, mfu 0.43%
iter 59: loss 0.4164, time 3223.16ms, mfu 0.43%
step 60: classification- train loss 0.5214, classification-val loss 0.5897
[WANDB] Logging at iter 60: train 0.5213546752929688, val 0.5897285342216492
saving checkpoint to out-sentiment-small
iter 60: loss 0.6370, time 4660.64ms, mfu 0.42%
iter 61: loss 0.3604, time 3179.07ms, mfu 0.42%
iter 62: loss 0.6743, time 3258.75ms, mfu 0.42%
iter 63: loss 0.3681, time 3191.32ms, mfu 0.43%
iter 64: loss 0.5308, time 3220.19ms, mfu 0.43%
iter 65: loss 0.2946, time 3322.10ms, mfu 0.43%
iter 66: loss 0.5186, time 3207.77ms, mfu 0.43%
iter 67: loss 0.2206, time 3255.76ms, mfu 0.43%
iter 68: loss 0.3685, time 3200.35ms, mfu 0.43%
iter 69: loss 0.4544, time 3236.85ms, mfu 0.43%
step 70: classification- train loss 0.4308, classification-val loss 0.5157
[WANDB] Logging at iter 70: train 0.43077316880226135, val 0.5157024264335632
saving checkpoint to out-sentiment-small
iter 70: loss 0.3478, time 4850.45ms, mfu 0.42%
iter 71: loss 0.5403, time 3268.09ms, mfu 0.42%
iter 72: loss 0.6801, time 3324.52ms, mfu 0.42%
iter 73: loss 0.5262, time 3234.90ms, mfu 0.42%
iter 74: loss 0.5750, time 3257.75ms, mfu 0.42%
iter 75: loss 0.4116, time 3232.59ms, mfu 0.42%
iter 76: loss 0.7757, time 3363.47ms, mfu 0.42%
iter 77: loss 0.3419, time 3285.00ms, mfu 0.42%
iter 78: loss 0.6878, time 3207.44ms, mfu 0.43%
iter 79: loss 0.2836, time 3222.63ms, mfu 0.43%
step 80: classification- train loss 0.3528, classification-val loss 0.4594
[WANDB] Logging at iter 80: train 0.35279160737991333, val 0.4594240188598633
saving checkpoint to out-sentiment-small
iter 80: loss 0.3485, time 5022.90ms, mfu 0.41%
iter 81: loss 0.2414, time 3287.05ms, mfu 0.41%
iter 82: loss 0.4529, time 3222.59ms, mfu 0.42%
iter 83: loss 0.4730, time 3213.68ms, mfu 0.42%
iter 84: loss 0.3585, time 3266.36ms, mfu 0.42%
iter 85: loss 0.2623, time 3228.34ms, mfu 0.42%
iter 86: loss 0.2317, time 3234.21ms, mfu 0.42%
iter 87: loss 0.3527, time 3290.94ms, mfu 0.42%
iter 88: loss 0.4089, time 3233.99ms, mfu 0.43%
iter 89: loss 0.2027, time 3269.52ms, mfu 0.43%
step 90: classification- train loss 0.3495, classification-val loss 0.5314
[WANDB] Logging at iter 90: train 0.3494857847690582, val 0.5313909649848938
saving checkpoint to out-sentiment-small
iter 90: loss 0.4783, time 4750.37ms, mfu 0.41%
iter 91: loss 0.4653, time 3219.96ms, mfu 0.42%
iter 92: loss 0.2569, time 3304.56ms, mfu 0.42%
iter 93: loss 0.2654, time 3245.08ms, mfu 0.42%
iter 94: loss 0.6880, time 3228.55ms, mfu 0.42%
iter 95: loss 0.6519, time 3284.42ms, mfu 0.42%
iter 96: loss 0.7935, time 3377.44ms, mfu 0.42%
iter 97: loss 0.2588, time 3495.61ms, mfu 0.42%
iter 98: loss 0.3136, time 3254.26ms, mfu 0.42%
iter 99: loss 0.3664, time 3269.90ms, mfu 0.42%
step 100: classification- train loss 0.3168, classification-val loss 0.4593
[WANDB] Logging at iter 100: train 0.3167858123779297, val 0.4593368470668793
saving checkpoint to out-sentiment-small
Traceback (most recent call last):
  File "C:\Users\delfi\Desktop\MMI\DI725\DI725-main\DI725-main\assignment_1\train.py", line 376, in <module>
    logits, loss = model(X, Y)
                   ^^^^^^^^^^^
  File "C:\Users\delfi\anaconda3\envs\normal\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\delfi\anaconda3\envs\normal\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\delfi\Desktop\MMI\DI725\DI725-main\DI725-main\assignment_1\model.py", line 186, in forward
    x = block(x)
        ^^^^^^^^
  File "C:\Users\delfi\anaconda3\envs\normal\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\delfi\anaconda3\envs\normal\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\delfi\Desktop\MMI\DI725\DI725-main\DI725-main\assignment_1\model.py", line 104, in forward
    x = x + self.attn(self.ln_1(x))
            ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\delfi\anaconda3\envs\normal\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\delfi\anaconda3\envs\normal\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\delfi\Desktop\MMI\DI725\DI725-main\DI725-main\assignment_1\model.py", line 64, in forward
    y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
